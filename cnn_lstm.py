# ventilation_diagnosis_complete.py
import os
import logging
import warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, \
    f1_score, roc_auc_score, roc_curve
from sklearn.utils.class_weight import compute_class_weight
import networkx as nx
import joblib
from collections import Counter
import traceback


# ==================== ç¯å¢ƒé…ç½® ====================
class VentilationEnvironment:
    """çŸ¿äº•é€šé£è¯Šæ–­ç³»ç»Ÿç¯å¢ƒé…ç½®"""

    @staticmethod
    def setup():
        """
        å®Œå…¨é…ç½®TensorFlowè¿è¡Œç¯å¢ƒ
        å±è”½æ‰€æœ‰ä¸å¿…è¦çš„ä¿¡æ¯æç¤ºï¼ŒåŒæ—¶ä¿æŒæ€§èƒ½ä¼˜åŒ–
        """
        # 1. è®¾ç½®ç¯å¢ƒå˜é‡ - åœ¨å¯¼å…¥TensorFlowä¹‹å‰å¿…é¡»å®Œæˆ
        os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # åªæ˜¾ç¤ºERRORçº§åˆ«ä¿¡æ¯
        os.environ['TF_ENABLE_ONEDNN_OPTS'] = '1'  # å¯ç”¨oneDNNä¼˜åŒ–ï¼ˆä¿æŒæ€§èƒ½ï¼‰
        os.environ['OMP_NUM_THREADS'] = '1'  # æ§åˆ¶çº¿ç¨‹æ•°

        # 2. å±è”½æ‰€æœ‰Pythonè­¦å‘Š
        warnings.filterwarnings('ignore')

        # 3. é…ç½®æ—¥å¿—ç³»ç»Ÿ - åœ¨å¯¼å…¥TensorFlowä¹‹å‰è®¾ç½®
        logging.basicConfig(level=logging.INFO)
        for logger_name in ['tensorflow', 'h5py', 'matplotlib']:
            logging.getLogger(logger_name).setLevel(logging.ERROR)

        # 4. é…ç½®matplotlibä¸­æ–‡å­—ä½“æ”¯æŒ
        VentilationEnvironment._setup_matplotlib()

        print("ğŸ”§ ç¯å¢ƒé…ç½®å®Œæˆ - å·²å±è”½TensorFlowä¿¡æ¯æç¤ºï¼Œå¯ç”¨äº†CPUä¼˜åŒ–")

    @staticmethod
    def _setup_matplotlib():
        """é…ç½®matplotlibä¸­æ–‡å­—ä½“æ”¯æŒ"""
        try:
            # è®¾ç½®matplotlibå‚æ•°ä»¥æ”¯æŒä¸­æ–‡æ˜¾ç¤º
            plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']  # ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºä¸­æ–‡æ ‡ç­¾
            plt.rcParams['axes.unicode_minus'] = False  # ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºè´Ÿå·
            plt.rcParams['figure.dpi'] = 100  # è®¾ç½®å›¾å½¢åˆ†è¾¨ç‡
            plt.rcParams['savefig.dpi'] = 300  # è®¾ç½®ä¿å­˜å›¾åƒçš„åˆ†è¾¨ç‡
            plt.rcParams['font.size'] = 12  # è®¾ç½®å­—ä½“å¤§å°

            # æµ‹è¯•ä¸­æ–‡å­—ä½“æ˜¯å¦å¯ç”¨
            import matplotlib.font_manager as fm
            test_fonts = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
            available_fonts = []

            for font in test_fonts:
                if any(font in f.name for f in fm.fontManager.ttflist):
                    available_fonts.append(font)

            if available_fonts:
                plt.rcParams['font.sans-serif'] = available_fonts + ['DejaVu Sans']
                print(f" ä¸­æ–‡å­—ä½“é…ç½®æˆåŠŸ: ä½¿ç”¨ {available_fonts[0]}")
            else:
                print("ï¸ æœªæ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œä½¿ç”¨é»˜è®¤å­—ä½“")

        except Exception as e:
            print(f"ï¸ å­—ä½“é…ç½®è­¦å‘Š: {e}")
            # è®¾ç½®å›é€€å­—ä½“é…ç½®
            plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial']
            plt.rcParams['axes.unicode_minus'] = False


# åº”ç”¨ç¯å¢ƒé…ç½®ï¼ˆå¿…é¡»åœ¨å¯¼å…¥TensorFlowä¹‹å‰ï¼‰
VentilationEnvironment.setup()

# ==================== å¯¼å…¥TensorFlowå’Œå…¶ä»–åº“ ====================
import tensorflow as tf
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import *
from tensorflow.keras.optimizers import Adam, RMSprop
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, LearningRateScheduler
from tensorflow.keras.utils import plot_model
from tensorflow.keras.regularizers import l2, l1_l2
from tensorflow.keras.constraints import MaxNorm

# è®¾ç½®éšæœºç§å­ä¿è¯å¯é‡å¤æ€§
tf.random.set_seed(42)
np.random.seed(42)

print(f" TensorFlowç‰ˆæœ¬: {tf.__version__}")


# ==================== çŸ¿äº•æ‹“æ‰‘ç»“æ„ç±» ====================
class MineTopology:
    """çŸ¿äº•å··é“æ‹“æ‰‘ç»“æ„ç®¡ç†ç±»"""

    def __init__(self):
        self.tunnels = {}
        self.graph = nx.DiGraph()
        self.tunnel_sequence = []  # æ‹“æ‰‘æ’åºç»“æœ
        self.resistance_matrix = None  # é£é˜»å½±å“çŸ©é˜µ

    def initialize_standard_topology(self):
        """åˆå§‹åŒ–æ ‡å‡†çŸ¿äº•æ‹“æ‰‘ç»“æ„ - ä½¿ç”¨e1,e2,e3...å‘½å"""
        # å®šä¹‰å··é“åŠå…¶è¿æ¥å…³ç³» - ä½¿ç”¨e1,e2,e3...å‘½å
        tunnels = {
            'e1': {'type': 'å…¥å£', 'level': 0, 'name': 'ä¸»äº•å£'},
            'e2': {'type': 'ä¸»å··é“', 'level': 1, 'name': 'è¿è¾“å¤§å··1'},
            'e3': {'type': 'ä¸»å··é“', 'level': 1, 'name': 'è¿è¾“å¤§å··2'},
            'e4': {'type': 'è¿æ¥å··é“', 'level': 2, 'name': 'é‡‡åŒºä¸Šå±±'},
            'e5': {'type': 'å·¥ä½œé¢', 'level': 3, 'name': 'å·¥ä½œé¢å··é“1'},
            'e6': {'type': 'å·¥ä½œé¢', 'level': 3, 'name': 'å·¥ä½œé¢å··é“2'},
            'e7': {'type': 'å·¥ä½œé¢', 'level': 3, 'name': 'å·¥ä½œé¢å··é“3'},
            'e8': {'type': 'å›é£', 'level': 2, 'name': 'å›é£å··1'},
            'e9': {'type': 'å›é£', 'level': 2, 'name': 'å›é£å··2'},
            'e10': {'type': 'å‡ºå£', 'level': 0, 'name': 'å›é£äº•'}
        }

        # å®šä¹‰è¿æ¥å…³ç³» (ä» -> åˆ°)
        connections = [
            ('e1', 'e2'),
            ('e1', 'e3'),
            ('e2', 'e4'),
            ('e3', 'e4'),
            ('e4', 'e5'),
            ('e4', 'e6'),
            ('e4', 'e7'),
            ('e5', 'e8'),
            ('e6', 'e8'),
            ('e7', 'e9'),
            ('e8', 'e10'),
            ('e9', 'e10')
        ]

        self.tunnels = tunnels
        self.graph.add_nodes_from(tunnels.keys())
        self.graph.add_edges_from(connections)

        # æ‰§è¡Œæ‹“æ‰‘æ’åº
        try:
            self.tunnel_sequence = list(nx.topological_sort(self.graph))
        except nx.NetworkXError:
            # å¦‚æœå›¾æœ‰ç¯ï¼Œä½¿ç”¨å…¶ä»–æ’åºæ–¹æ³•
            self.tunnel_sequence = list(self.tunnels.keys())

        # è®¡ç®—é£é˜»å½±å“çŸ©é˜µ
        self._calculate_resistance_influence_matrix()

        print(" çŸ¿äº•æ‹“æ‰‘ç»“æ„åˆå§‹åŒ–å®Œæˆ")
        print(f" å··é“æ•°é‡: {len(self.tunnels)}")
        print(f" è¿æ¥æ•°é‡: {len(connections)}")
        print(f" æ‹“æ‰‘æ’åºç»“æœ: {self.tunnel_sequence}")

        # æ‰“å°å··é“è¯¦ç»†ä¿¡æ¯
        print("\n å··é“è¯¦ç»†ä¿¡æ¯:")
        for tunnel_id, info in self.tunnels.items():
            print(f"  {tunnel_id}: {info['name']} ({info['type']}, å±‚çº§: {info['level']})")

        return self.tunnels, self.graph

    def _calculate_resistance_influence_matrix(self):
        """è®¡ç®—é£é˜»å½±å“çŸ©é˜µ - è¡¨ç¤ºå··é“é—´é£é˜»å˜åŒ–çš„ç›¸äº’å½±å“"""
        n_tunnels = len(self.tunnels)
        tunnel_names = list(self.tunnels.keys())

        # åˆå§‹åŒ–å½±å“çŸ©é˜µ
        influence_matrix = np.zeros((n_tunnels, n_tunnels))

        # åŸºäºç½‘ç»œæ‹“æ‰‘è®¡ç®—å½±å“ç³»æ•°
        for i, tunnel_i in enumerate(tunnel_names):
            for j, tunnel_j in enumerate(tunnel_names):
                if i == j:
                    # è‡ªèº«å½±å“æœ€å¤§
                    influence_matrix[i, j] = 1.0
                else:
                    # è®¡ç®—æ‹“æ‰‘è·ç¦»å½±å“
                    try:
                        # è®¡ç®—æœ€çŸ­è·¯å¾„è·ç¦»
                        distance = nx.shortest_path_length(self.graph, tunnel_i, tunnel_j)
                        # è·ç¦»è¶Šè¿‘ï¼Œå½±å“è¶Šå¤§
                        influence_matrix[i, j] = 0.5 / distance
                    except:
                        # å¦‚æœä¸å¯è¾¾ï¼Œå½±å“ä¸º0
                        influence_matrix[i, j] = 0.0

        self.resistance_matrix = influence_matrix
        print(" é£é˜»å½±å“çŸ©é˜µè®¡ç®—å®Œæˆ")
        return influence_matrix

    def get_tunnel_features(self, tunnel_name):
        """è·å–å··é“çš„ç‰¹å¾å‘é‡"""
        tunnel_info = self.tunnels.get(tunnel_name, {})
        features = {
            'level': tunnel_info.get('level', 0),
            'is_entrance': 1 if tunnel_info.get('type') == 'å…¥å£' else 0,
            'is_exit': 1 if tunnel_info.get('type') == 'å‡ºå£' else 0,
            'is_workface': 1 if tunnel_info.get('type') == 'å·¥ä½œé¢' else 0,
            'is_main': 1 if tunnel_info.get('type') == 'ä¸»å··é“' else 0,
            'is_ventilation': 1 if tunnel_info.get('type') == 'å›é£' else 0,
            'connectivity': self.graph.degree(tunnel_name) if tunnel_name in self.graph else 0
        }
        return features

    def calculate_wind_resistance(self, wind_speeds, pressures, cross_sections):
        """
        æ ¹æ®é£é€Ÿã€é£å‹å’Œæ–­é¢é¢ç§¯è®¡ç®—é£é˜»
        é£é˜» R = Î”P / (Ï * v^2 * A^2)
        å…¶ä¸­: Î”P - é£å‹å·®, Ï - ç©ºæ°”å¯†åº¦, v - é£é€Ÿ, A - æ–­é¢é¢ç§¯
        """
        air_density = 1.2  # ç©ºæ°”å¯†åº¦ kg/mÂ³

        resistances = {}
        for tunnel in self.tunnels.keys():
            if tunnel in wind_speeds and tunnel in pressures and tunnel in cross_sections:
                v = wind_speeds[tunnel]
                P = pressures[tunnel]
                A = cross_sections[tunnel]

                if v > 0 and A > 0:
                    # è®¡ç®—é£é˜»
                    resistance = P / (air_density * v ** 2 * A ** 2)
                else:
                    resistance = 0.0

                resistances[tunnel] = resistance

        return resistances

    def simulate_resistance_effect(self, original_resistances, changed_tunnel, change_factor):
        """
        æ¨¡æ‹Ÿä¸€æ¡å··é“é£é˜»å˜åŒ–å¯¹å…¶ä»–å··é“é£é˜»çš„å½±å“
        """
        tunnel_names = list(self.tunnels.keys())
        changed_idx = tunnel_names.index(changed_tunnel)

        # è®¡ç®—å½±å“å‘é‡
        influence_vector = self.resistance_matrix[changed_idx, :]

        # è®¡ç®—æ–°çš„é£é˜»å€¼
        new_resistances = original_resistances.copy()
        for i, tunnel in enumerate(tunnel_names):
            if tunnel == changed_tunnel:
                # æ•…éšœå··é“çš„é£é˜»ç›´æ¥å˜åŒ–
                new_resistances[tunnel] *= change_factor
            else:
                # å…¶ä»–å··é“å—å½±å“çš„é¢¨é˜»å˜åŒ–
                influence = influence_vector[i]
                resistance_change = (change_factor - 1.0) * influence * 0.3  # è¡°å‡ç³»æ•°
                new_resistances[tunnel] *= (1.0 + resistance_change)

        return new_resistances

    def visualize_topology(self):
        """å¯è§†åŒ–çŸ¿äº•æ‹“æ‰‘ç»“æ„"""
        plt.figure(figsize=(15, 10))

        # ä½¿ç”¨å±‚æ¬¡å¸ƒå±€
        pos = {}
        level_nodes = {}

        # æŒ‰å±‚çº§åˆ†ç»„èŠ‚ç‚¹
        for node, attrs in self.tunnels.items():
            level = attrs['level']
            if level not in level_nodes:
                level_nodes[level] = []
            level_nodes[level].append(node)

        # ä¸ºæ¯ä¸ªå±‚çº§çš„èŠ‚ç‚¹åˆ†é…ä½ç½®
        for level, nodes in level_nodes.items():
            n_nodes = len(nodes)
            for i, node in enumerate(nodes):
                pos[node] = (i - n_nodes / 2, -level)

        # æ ¹æ®èŠ‚ç‚¹ç±»å‹è®¾ç½®é¢œè‰²
        node_colors = []
        node_labels = {}
        for node in self.graph.nodes():
            node_type = self.tunnels[node]['type']
            node_labels[node] = f"{node}\n{self.tunnels[node]['name']}"

            if node_type == 'å…¥å£':
                node_colors.append('lightgreen')
            elif node_type == 'å‡ºå£':
                node_colors.append('lightcoral')
            elif node_type == 'å·¥ä½œé¢':
                node_colors.append('lightblue')
            elif node_type == 'ä¸»å··é“':
                node_colors.append('yellow')
            else:
                node_colors.append('lightgray')

        # ç»˜åˆ¶å›¾å½¢
        nx.draw(self.graph, pos,
                labels=node_labels,
                node_color=node_colors,
                node_size=2000,
                font_size=8,
                font_weight='bold',
                arrows=True,
                arrowsize=20,
                edge_color='gray',
                edgecolors='black',
                linewidths=1)

        plt.title('çŸ¿äº•é€šé£ç³»ç»Ÿæ‹“æ‰‘ç»“æ„å›¾ (e1-e10å··é“ç¼–å·)', fontsize=16, fontweight='bold')
        plt.tight_layout()

        # æ˜¾ç¤ºå›¾ä¾‹
        legend_elements = [
            plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='lightgreen', markersize=10, label='å…¥å£'),
            plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='yellow', markersize=10, label='ä¸»å··é“'),
            plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='lightblue', markersize=10, label='å·¥ä½œé¢'),
            plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='lightgray', markersize=10, label='è¿æ¥å··é“'),
            plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='lightcoral', markersize=10, label='å‡ºå£')
        ]
        plt.legend(handles=legend_elements, loc='upper left')

        # å…ˆæ˜¾ç¤ºå†ä¿å­˜
        plt.show()

        # ä¿å­˜æ‹“æ‰‘å›¾
        save_path = "D:/Project_python/mine_topology.png"
        plt.savefig(save_path, dpi=300, bbox_inches='tight',
                    facecolor='white', edgecolor='none')
        print(f" æ‹“æ‰‘ç»“æ„å›¾å·²ä¿å­˜ä¸º '{save_path}'")


# ==================== æ•°æ®å¤„ç†å™¨ - åŸºäºé£é€Ÿè®¡ç®—é£é˜» ====================
class VentilationDataProcessor:
    """çŸ¿äº•é€šé£æ•°æ®é¢„å¤„ç†ç±» - åŸºäºé£é€Ÿè®¡ç®—é£é˜»çš„æ•…éšœè¯Šæ–­"""

    def __init__(self, sequence_length=60):
        self.sequence_length = sequence_length
        self.scaler = RobustScaler()
        self.label_encoder = LabelEncoder()
        self.tunnel_encoder = LabelEncoder()
        self.feature_names = None
        self.is_fitted = False
        self.data_file_path = "D:/Project_python/wind_speed_resistance_data.xls"
        self.topology = MineTopology()
        self.tunnels, self.graph = self.topology.initialize_standard_topology()

    def load_data(self):
        """åŠ è½½åŸºäºé£é€Ÿè®¡ç®—é£é˜»çš„ä¼ æ„Ÿå™¨æ•°æ®"""
        try:
            if os.path.exists(self.data_file_path):
                print(f" ä»ç»å¯¹è·¯å¾„åŠ è½½é£é€Ÿé£é˜»æ•°æ®: {self.data_file_path}")
                data = pd.read_excel(self.data_file_path)
                print(f" æ•°æ®åŠ è½½æˆåŠŸï¼Œå½¢çŠ¶: {data.shape}")
                return data
            else:
                print(f"ï¸ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {self.data_file_path}")
                print(" ç”ŸæˆåŸºäºé£é€Ÿè®¡ç®—é£é˜»çš„ç¤ºä¾‹æ•°æ®")
                data = self._create_wind_speed_resistance_sample_data()

                # ç¡®ä¿ç›®å½•å­˜åœ¨
                os.makedirs(os.path.dirname(self.data_file_path), exist_ok=True)
                data.to_excel(self.data_file_path, index=False)
                print(f" ç¤ºä¾‹æ•°æ®å·²ä¿å­˜åˆ°: {self.data_file_path}")
                return data

        except Exception as e:
            print(f" æ•°æ®åŠ è½½å¤±è´¥: {e}")
            print(" ä½¿ç”¨ç”Ÿæˆçš„ç¤ºä¾‹æ•°æ®")
            return self._create_wind_speed_resistance_sample_data()

    def _create_wind_speed_resistance_sample_data(self):
        """åˆ›å»ºåŸºäºé£é€Ÿè®¡ç®—é£é˜»çš„æ ·æœ¬æ•°æ®"""
        np.random.seed(42)
        n_samples = 12000
        tunnel_names = list(self.tunnels.keys())
        n_tunnels = len(tunnel_names)

        # åˆå§‹åŒ–æ•°æ®å­˜å‚¨
        all_data = []
        labels = []
        fault_tunnels = []

        # å®šä¹‰å··é“åŸºæœ¬å‚æ•°
        base_wind_speeds = {}
        base_pressures = {}
        cross_sections = {}

        # ä¸ºæ¯ä¸ªå··é“è®¾ç½®åŸºæœ¬å‚æ•°
        for tunnel in tunnel_names:
            tunnel_info = self.topology.get_tunnel_features(tunnel)
            # åŸºç¡€é£é€Ÿä¸å··é“ç±»å‹ç›¸å…³
            if tunnel_info['is_entrance'] or tunnel_info['is_exit']:
                base_wind_speeds[tunnel] = 8.0 + np.random.uniform(-1, 1)
            elif tunnel_info['is_main']:
                base_wind_speeds[tunnel] = 6.0 + np.random.uniform(-0.8, 0.8)
            elif tunnel_info['is_workface']:
                base_wind_speeds[tunnel] = 4.0 + np.random.uniform(-0.5, 0.5)
            else:
                base_wind_speeds[tunnel] = 5.0 + np.random.uniform(-0.6, 0.6)

            # åŸºç¡€é£å‹
            base_pressures[tunnel] = 1000 + tunnel_info['level'] * 50 + np.random.uniform(-20, 20)

            # æ–­é¢é¢ç§¯ (mÂ²)
            if tunnel_info['is_main']:
                cross_sections[tunnel] = 12.0 + np.random.uniform(-1, 1)
            elif tunnel_info['is_workface']:
                cross_sections[tunnel] = 8.0 + np.random.uniform(-0.8, 0.8)
            else:
                cross_sections[tunnel] = 10.0 + np.random.uniform(-1, 1)

        # æ­£å¸¸çŠ¶æ€çš„åŸºç¡€é£é˜»
        base_resistances = self.topology.calculate_wind_resistance(
            base_wind_speeds, base_pressures, cross_sections
        )

        # ç”Ÿæˆæ—¶é—´åºåˆ—æ•°æ®
        time = np.linspace(0, 200, n_samples)

        for i in range(n_samples):
            # åŸºç¡€é£é€Ÿæ³¢åŠ¨
            current_wind_speeds = base_wind_speeds.copy()
            current_pressures = base_pressures.copy()

            # æ·»åŠ æ­£å¸¸æ³¢åŠ¨
            for tunnel in tunnel_names:
                # å‘¨æœŸæ€§æ³¢åŠ¨
                periodic = 0.3 * np.sin(2 * np.pi * 0.01 * time[i] + hash(tunnel) % 10)
                # è¶‹åŠ¿æ€§å˜åŒ–
                trend = 0.0001 * time[i]
                # éšæœºå™ªå£°
                noise = 0.1 * np.random.randn()

                current_wind_speeds[tunnel] = base_wind_speeds[tunnel] + periodic + trend + noise
                current_pressures[tunnel] = base_pressures[tunnel] + 10 * periodic + 5 * noise

            # ç¡®å®šçŠ¶æ€å’Œæ•…éšœ
            if i < 6000:
                # æ­£å¸¸çŠ¶æ€
                labels.append('æ­£å¸¸')
                fault_tunnels.append('æ— æ•…éšœ')
                current_resistances = base_resistances.copy()
            else:
                # æ•…éšœçŠ¶æ€
                labels.append('æ•…éšœ')
                # éšæœºé€‰æ‹©æ•…éšœå··é“
                fault_tunnel = np.random.choice(tunnel_names)
                fault_tunnels.append(fault_tunnel)

                # æ•…éšœå¼ºåº¦
                fault_intensity = np.random.uniform(1.5, 3.0)  # é£é˜»å¢åŠ å€æ•°

                # æ¨¡æ‹Ÿé£é˜»å˜åŒ–åŠå…¶å¯¹ç³»ç»Ÿçš„å½±å“
                current_resistances = self.topology.simulate_resistance_effect(
                    base_resistances, fault_tunnel, fault_intensity
                )

                # æ ¹æ®æ–°çš„é£é˜»è°ƒæ•´é£é€Ÿå’Œé£å‹ï¼ˆç®€åŒ–æ¨¡æ‹Ÿï¼‰
                for tunnel in tunnel_names:
                    resistance_change_ratio = current_resistances[tunnel] / base_resistances[tunnel]
                    # é£é˜»å¢åŠ ä¼šå¯¼è‡´é£é€Ÿä¸‹é™
                    current_wind_speeds[tunnel] /= np.sqrt(resistance_change_ratio)
                    # é£å‹ç›¸åº”è°ƒæ•´
                    current_pressures[tunnel] *= (1 + 0.1 * (resistance_change_ratio - 1))

            # è®¡ç®—å½“å‰æ—¶é—´æ­¥çš„ç‰¹å¾
            sample_features = []

            # 1. å„å··é“é£é€Ÿç‰¹å¾
            for tunnel in tunnel_names:
                sample_features.extend([
                    current_wind_speeds[tunnel],
                    current_pressures[tunnel],
                    cross_sections[tunnel]
                ])

            # 2. è®¡ç®—å¹¶æ·»åŠ é£é˜»ç‰¹å¾
            calculated_resistances = self.topology.calculate_wind_resistance(
                current_wind_speeds, current_pressures, cross_sections
            )

            for tunnel in tunnel_names:
                sample_features.append(calculated_resistances.get(tunnel, 0.0))

            # 3. æ·»åŠ é£é€Ÿå˜åŒ–ç‡ç‰¹å¾
            if i > 0:
                for tunnel in tunnel_names:
                    wind_speed_change = current_wind_speeds[tunnel] - all_data[i - 1][tunnel_names.index(tunnel) * 3]
                    sample_features.append(wind_speed_change)
            else:
                for _ in tunnel_names:
                    sample_features.append(0.0)

            # 4. æ·»åŠ æ‹“æ‰‘ç‰¹å¾
            for tunnel in tunnel_names:
                tunnel_features = self.topology.get_tunnel_features(tunnel)
                sample_features.extend([
                    tunnel_features['level'],
                    tunnel_features['connectivity'],
                    tunnel_features['is_workface']
                ])

            all_data.append(sample_features)

        # åˆ›å»ºç‰¹å¾åç§°
        feature_names = []

        # é£é€Ÿã€é£å‹ã€æ–­é¢é¢ç§¯ç‰¹å¾
        for tunnel in tunnel_names:
            tunnel_name = self.tunnels[tunnel]['name']
            feature_names.extend([
                f'{tunnel}_{tunnel_name}_é£é€Ÿ',
                f'{tunnel}_{tunnel_name}_é£å‹',
                f'{tunnel}_{tunnel_name}_æ–­é¢é¢ç§¯'
            ])

        # é£é˜»ç‰¹å¾
        for tunnel in tunnel_names:
            tunnel_name = self.tunnels[tunnel]['name']
            feature_names.append(f'{tunnel}_{tunnel_name}_é£é˜»')

        # é£é€Ÿå˜åŒ–ç‡ç‰¹å¾
        for tunnel in tunnel_names:
            tunnel_name = self.tunnels[tunnel]['name']
            feature_names.append(f'{tunnel}_{tunnel_name}_é£é€Ÿå˜åŒ–ç‡')

        # æ‹“æ‰‘ç‰¹å¾
        for tunnel in tunnel_names:
            feature_names.extend([
                f'{tunnel}_å±‚çº§',
                f'{tunnel}_è¿é€šåº¦',
                f'{tunnel}_æ˜¯å¦å·¥ä½œé¢'
            ])

        # åˆ›å»ºDataFrame
        df = pd.DataFrame(all_data, columns=feature_names)
        df['çŠ¶æ€'] = labels
        df['æ•…éšœå··é“'] = fault_tunnels
        df['æ—¶é—´æˆ³'] = pd.date_range(start='2024-01-01', periods=n_samples, freq='min')

        print(f" åŸºäºé£é€Ÿè®¡ç®—é£é˜»çš„æ•°æ®ç”Ÿæˆå®Œæˆ: {df.shape}")
        status_distribution = df['çŠ¶æ€'].value_counts()
        fault_tunnel_distribution = df[df['çŠ¶æ€'] == 'æ•…éšœ']['æ•…éšœå··é“'].value_counts()

        print(f" çŠ¶æ€åˆ†å¸ƒ: {status_distribution.to_dict()}")
        print(f" æ•…éšœå··é“åˆ†å¸ƒ: {fault_tunnel_distribution.to_dict()}")

        return df

    def preprocess_data(self, data, test_size=0.15, val_size=0.15):
        """æ•°æ®é¢„å¤„ç† - åŸºäºé£é€Ÿé£é˜»ç‰¹å¾"""
        try:
            # åˆ†ç¦»ç‰¹å¾å’Œæ ‡ç­¾
            feature_cols = [col for col in data.columns if col not in ['çŠ¶æ€', 'æ•…éšœå··é“', 'æ—¶é—´æˆ³']]
            self.feature_names = feature_cols

            X = data[feature_cols].values
            y_status = data['çŠ¶æ€'].values
            y_tunnel = data['æ•…éšœå··é“'].values

            # æ ‡å‡†åŒ–ç‰¹å¾
            X_scaled = self.scaler.fit_transform(X)

            # ç¼–ç æ ‡ç­¾
            y_status_encoded = self.label_encoder.fit_transform(y_status)
            y_tunnel_encoded = self.tunnel_encoder.fit_transform(y_tunnel)

            self.is_fitted = True

            print(f" æ•°æ®é¢„å¤„ç†å®Œæˆ - ç‰¹å¾: {X_scaled.shape}")
            print(f" çŠ¶æ€ç±»åˆ«: {list(self.label_encoder.classes_)}")
            print(f" å··é“ç±»åˆ«: {list(self.tunnel_encoder.classes_)}")

            # è®¡ç®—ç±»åˆ«æƒé‡ç”¨äºå‚è€ƒ
            status_class_weights = compute_class_weight(
                'balanced',
                classes=np.unique(y_status_encoded),
                y=y_status_encoded
            )
            self.status_class_weights = dict(enumerate(status_class_weights))

            tunnel_class_weights = compute_class_weight(
                'balanced',
                classes=np.unique(y_tunnel_encoded),
                y=y_tunnel_encoded
            )
            self.tunnel_class_weights = dict(enumerate(tunnel_class_weights))

            print(f"ï¸ çŠ¶æ€ç±»åˆ«æƒé‡: {self.status_class_weights}")
            print(f"ï¸ å··é“ç±»åˆ«æƒé‡: {self.tunnel_class_weights}")

            return X_scaled, y_status_encoded, y_tunnel_encoded

        except Exception as e:
            print(f" æ•°æ®é¢„å¤„ç†å¤±è´¥: {e}")
            raise

    def create_sequences(self, X, y_status, y_tunnel, step_size=5):
        """åˆ›å»ºæ—¶é—´åºåˆ—æ•°æ®"""
        if not self.is_fitted:
            raise ValueError("æ•°æ®å¤„ç†å™¨å°šæœªæ‹Ÿåˆï¼Œè¯·å…ˆè°ƒç”¨preprocess_dataæ–¹æ³•")

        sequences = []
        status_labels = []
        tunnel_labels = []

        for i in range(0, len(X) - self.sequence_length + 1, step_size):
            sequences.append(X[i:(i + self.sequence_length)])
            status_labels.append(y_status[i + self.sequence_length - 1])
            tunnel_labels.append(y_tunnel[i + self.sequence_length - 1])

        sequences = np.array(sequences)
        status_labels = np.array(status_labels)
        tunnel_labels = np.array(tunnel_labels)

        print(f" åºåˆ—æ•°æ®åˆ›å»ºå®Œæˆ - åºåˆ—å½¢çŠ¶: {sequences.shape}")
        print(f" çŠ¶æ€æ ‡ç­¾å½¢çŠ¶: {status_labels.shape}")
        print(f" å··é“æ ‡ç­¾å½¢çŠ¶: {tunnel_labels.shape}")

        return sequences, status_labels, tunnel_labels

    def save_preprocessor(self, file_path=None):
        """ä¿å­˜é¢„å¤„ç†å™¨çŠ¶æ€"""
        if file_path is None:
            preprocessor_dir = "D:/Project_python"
            os.makedirs(preprocessor_dir, exist_ok=True)
            file_path = os.path.join(preprocessor_dir, "wind_speed_resistance_preprocessor.pkl")

        preprocessor_state = {
            'scaler': self.scaler,
            'label_encoder': self.label_encoder,
            'tunnel_encoder': self.tunnel_encoder,
            'sequence_length': self.sequence_length,
            'feature_names': self.feature_names,
            'is_fitted': self.is_fitted,
            'status_class_weights': getattr(self, 'status_class_weights', None),
            'tunnel_class_weights': getattr(self, 'tunnel_class_weights', None),
            'data_file_path': self.data_file_path
        }
        joblib.dump(preprocessor_state, file_path)
        print(f" é¢„å¤„ç†å™¨çŠ¶æ€å·²ä¿å­˜åˆ°: {file_path}")

    def load_preprocessor(self, file_path=None):
        """åŠ è½½é¢„å¤„ç†å™¨çŠ¶æ€"""
        if file_path is None:
            file_path = "D:/Project_python/wind_speed_resistance_preprocessor.pkl"

        try:
            preprocessor_state = joblib.load(file_path)
            self.scaler = preprocessor_state['scaler']
            self.label_encoder = preprocessor_state['label_encoder']
            self.tunnel_encoder = preprocessor_state['tunnel_encoder']
            self.sequence_length = preprocessor_state['sequence_length']
            self.feature_names = preprocessor_state['feature_names']
            self.is_fitted = preprocessor_state['is_fitted']
            self.status_class_weights = preprocessor_state.get('status_class_weights', None)
            self.tunnel_class_weights = preprocessor_state.get('tunnel_class_weights', None)
            self.data_file_path = preprocessor_state.get('data_file_path', self.data_file_path)
            print(f" é¢„å¤„ç†å™¨çŠ¶æ€å·²ä» {file_path} åŠ è½½")
        except Exception as e:
            print(f" é¢„å¤„ç†å™¨åŠ è½½å¤±è´¥: {e}")
            print(" ä½¿ç”¨æ–°çš„é¢„å¤„ç†å™¨")
            self.topology = MineTopology()
            self.tunnels, self.graph = self.topology.initialize_standard_topology()


# ==================== å¢å¼ºçš„å¤šä»»åŠ¡å­¦ä¹ æ¨¡å‹ ====================
class EnhancedMultiTaskCNNLSTMModel:
    """å¢å¼ºçš„å¤šä»»åŠ¡å­¦ä¹ CNN-LSTMæ··åˆæ¨¡å‹ - ä¸“é—¨å¤„ç†é£é˜»ä¼ æ’­æ•ˆåº”"""

    def __init__(self, input_shape, num_status_classes=2, num_tunnel_classes=11,
                 model_name="enhanced_ventilation_model"):
        self.input_shape = input_shape
        self.num_status_classes = num_status_classes
        self.num_tunnel_classes = num_tunnel_classes
        self.model_name = model_name
        self.model = None
        self.history = None
        self.label_encoder = None
        self.tunnel_encoder = None
        self.lr_history = []

    def build_enhanced_model(self, learning_rate=0.001):
        """æ„å»ºå¢å¼ºçš„å¤šä»»åŠ¡å­¦ä¹ æ¨¡å‹ï¼Œä¸“é—¨å¤„ç†é£é˜»ä¼ æ’­æ•ˆåº”"""
        try:
            # è¾“å…¥å±‚
            inputs = Input(shape=self.input_shape, name='input')

            # å¢å¼ºçš„ç‰¹å¾æå–å±‚ - ä½¿ç”¨æ›´æ·±çš„ç½‘ç»œæ•è·å¤æ‚æ¨¡å¼
            # ç¬¬ä¸€å·ç§¯å—
            x = Conv1D(filters=64, kernel_size=7, padding='same', activation='relu',
                       kernel_regularizer=l2(0.002))(inputs)
            x = BatchNormalization()(x)
            x = MaxPooling1D(pool_size=2)(x)
            x = SpatialDropout1D(0.2)(x)

            # ç¬¬äºŒå·ç§¯å—
            x = Conv1D(filters=128, kernel_size=5, padding='same', activation='relu',
                       kernel_regularizer=l2(0.002))(x)
            x = BatchNormalization()(x)
            x = MaxPooling1D(pool_size=2)(x)
            x = SpatialDropout1D(0.25)(x)

            # ç¬¬ä¸‰å·ç§¯å—
            x = Conv1D(filters=256, kernel_size=3, padding='same', activation='relu',
                       kernel_regularizer=l2(0.001))(x)
            x = BatchNormalization()(x)
            x = SpatialDropout1D(0.3)(x)

            # ç¬¬å››å·ç§¯å— - æ•è·æ›´ç»†å¾®çš„æ¨¡å¼
            x = Conv1D(filters=512, kernel_size=3, padding='same', activation='relu',
                       kernel_regularizer=l2(0.001))(x)
            x = BatchNormalization()(x)
            x = SpatialDropout1D(0.3)(x)

            # åŒå‘LSTMå±‚ - å¢å¼ºæ—¶åºç‰¹å¾æå–
            x = Bidirectional(LSTM(units=256, return_sequences=True,
                                   kernel_regularizer=l2(0.001),
                                   recurrent_regularizer=l2(0.001)))(x)
            x = Dropout(0.4)(x)

            x = Bidirectional(LSTM(units=128, return_sequences=True,
                                   kernel_regularizer=l2(0.001),
                                   recurrent_regularizer=l2(0.001)))(x)
            x = Dropout(0.4)(x)

            x = Bidirectional(LSTM(units=64, return_sequences=False,
                                   kernel_regularizer=l2(0.001),
                                   recurrent_regularizer=l2(0.001)))(x)
            x = Dropout(0.4)(x)

            # æ³¨æ„åŠ›æœºåˆ¶
            attention = Dense(64, activation='tanh')(x)
            attention_weights = Dense(1, activation='softmax')(attention)
            weighted_features = Multiply()([x, attention_weights])

            # å…±äº«çš„å¯†é›†å±‚
            shared_features = Dense(units=128, activation='relu', name='shared_features')(weighted_features)
            shared_features = Dropout(0.3)(shared_features)
            shared_features = Dense(units=64, activation='relu')(shared_features)
            shared_features = Dropout(0.2)(shared_features)

            # ä»»åŠ¡1ï¼šæ•…éšœè¯Šæ–­ï¼ˆäºŒåˆ†ç±»ï¼‰
            status_branch = Dense(units=32, activation='relu', name='status_branch')(shared_features)
            status_branch = Dropout(0.2)(status_branch)
            status_output = Dense(units=self.num_status_classes, activation='softmax', name='status_output')(
                status_branch)

            # ä»»åŠ¡2ï¼šæ•…éšœå··é“å®šä½ï¼ˆå¤šåˆ†ç±»ï¼‰
            tunnel_branch = Dense(units=64, activation='relu', name='tunnel_branch')(shared_features)
            tunnel_branch = Dropout(0.3)(tunnel_branch)
            tunnel_branch = Dense(units=32, activation='relu')(tunnel_branch)
            tunnel_branch = Dropout(0.2)(tunnel_branch)
            tunnel_output = Dense(units=self.num_tunnel_classes, activation='softmax', name='tunnel_output')(
                tunnel_branch)

            # åˆ›å»ºå¤šè¾“å‡ºæ¨¡å‹
            model = tf.keras.Model(
                inputs=inputs,
                outputs=[status_output, tunnel_output],
                name=self.model_name
            )

            # ç¼–è¯‘æ¨¡å‹
            optimizer = Adam(learning_rate=learning_rate, clipnorm=1.0)

            model.compile(
                optimizer=optimizer,
                loss={
                    'status_output': 'sparse_categorical_crossentropy',
                    'tunnel_output': 'sparse_categorical_crossentropy'
                },
                loss_weights={
                    'status_output': 0.6,  # æ•…éšœè¯Šæ–­ä»»åŠ¡æƒé‡
                    'tunnel_output': 0.4  # å··é“å®šä½ä»»åŠ¡æƒé‡
                },
                metrics={
                    'status_output': ['accuracy', 'precision', 'recall'],
                    'tunnel_output': ['accuracy']
                }
            )

            self.model = model
            print(" å¢å¼ºçš„å¤šä»»åŠ¡å­¦ä¹ æ¨¡å‹æ„å»ºå®Œæˆ")
            print(f" è¾“å…¥å½¢çŠ¶: {self.input_shape}")
            print(f" ä»»åŠ¡1 - æ•…éšœè¯Šæ–­: {self.num_status_classes}ç±»")
            print(f" ä»»åŠ¡2 - å··é“å®šä½: {self.num_tunnel_classes}ç±»")
            print(f" æ¨¡å‹æ€»å‚æ•°: {model.count_params():,}")

            return model

        except Exception as e:
            print(f" æ¨¡å‹æ„å»ºå¤±è´¥: {e}")
            raise

    def train_with_enhanced_validation(self, X_train, y_status_train, y_tunnel_train,
                                       X_val, y_status_val, y_tunnel_val,
                                       epochs=150, batch_size=32):
        """ä½¿ç”¨å¢å¼ºçš„è®­ç»ƒç­–ç•¥è®­ç»ƒæ¨¡å‹"""
        try:
            checkpoint_dir = "D:/Project_python/checkpoints"
            os.makedirs(checkpoint_dir, exist_ok=True)

            self.lr_history = []

            # å¢å¼ºçš„å›è°ƒå‡½æ•°
            callbacks = [
                EarlyStopping(
                    monitor='val_status_output_accuracy',
                    patience=25,
                    restore_best_weights=True,
                    verbose=1,
                    mode='max',
                    min_delta=0.001
                ),
                ReduceLROnPlateau(
                    monitor='val_loss',
                    factor=0.5,
                    patience=12,
                    min_lr=1e-7,
                    verbose=1,
                    mode='min',
                    min_delta=0.001
                ),
                ModelCheckpoint(
                    filepath=os.path.join(checkpoint_dir, f"{self.model_name}_best.keras"),
                    monitor='val_status_output_accuracy',
                    save_best_only=True,
                    save_weights_only=False,
                    verbose=1,
                    mode='max'
                ),
                # å­¦ä¹ ç‡è°ƒåº¦å™¨
                LearningRateScheduler(self._step_decay_schedule)
            ]

            print(f" å¼€å§‹å¢å¼ºçš„å¤šä»»åŠ¡å­¦ä¹ è®­ç»ƒ")
            print(f" è®­ç»ƒå‚æ•°: è½®æ•°={epochs}, æ‰¹æ¬¡å¤§å°={batch_size}")

            # è®­ç»ƒæ•°æ®å‡†å¤‡
            train_data = {
                'status_output': y_status_train,
                'tunnel_output': y_tunnel_train
            }

            val_data = {
                'status_output': y_status_val,
                'tunnel_output': y_tunnel_val
            }

            # è®­ç»ƒæ¨¡å‹
            self.history = self.model.fit(
                X_train, train_data,
                batch_size=batch_size,
                epochs=epochs,
                validation_data=(X_val, val_data),
                callbacks=callbacks,
                verbose=1,
                shuffle=True
            )

            print(" å¢å¼ºçš„å¤šä»»åŠ¡æ¨¡å‹è®­ç»ƒå®Œæˆ")
            return self.history

        except Exception as e:
            print(f" æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            raise

    def _step_decay_schedule(self, epoch, lr):
        """å­¦ä¹ ç‡è¡°å‡ç­–ç•¥"""
        if epoch > 0 and epoch % 30 == 0:
            new_lr = lr * 0.5
            print(f" å­¦ä¹ ç‡ä» {lr:.6f} é™ä½åˆ° {new_lr:.6f}")
            return new_lr
        return lr

    def comprehensive_evaluate(self, X_test, y_status_test, y_tunnel_test):
        """å…¨é¢è¯„ä¼°å¢å¼ºæ¨¡å‹æ€§èƒ½"""
        try:
            # é¢„æµ‹
            predictions = self.model.predict(X_test, verbose=0)
            status_pred_proba, tunnel_pred_proba = predictions
            status_pred = np.argmax(status_pred_proba, axis=1)
            tunnel_pred = np.argmax(tunnel_pred_proba, axis=1)

            # æ•…éšœè¯Šæ–­ä»»åŠ¡è¯„ä¼°
            status_accuracy = accuracy_score(y_status_test, status_pred)
            status_precision = precision_score(y_status_test, status_pred, average='binary', zero_division=0)
            status_recall = recall_score(y_status_test, status_pred, average='binary', zero_division=0)
            status_f1 = f1_score(y_status_test, status_pred, average='binary', zero_division=0)

            print(f" æ•…éšœè¯Šæ–­ä»»åŠ¡æ€§èƒ½:")
            print(f"   å‡†ç¡®ç‡: {status_accuracy:.4f}")
            print(f"   ç²¾ç¡®ç‡: {status_precision:.4f}")
            print(f"   å¬å›ç‡: {status_recall:.4f}")
            print(f"  ï¸ F1åˆ†æ•°: {status_f1:.4f}")

            # å··é“å®šä½ä»»åŠ¡è¯„ä¼°ï¼ˆåªè¯„ä¼°æ•…éšœæ ·æœ¬ï¼‰
            fault_mask = y_status_test == 1
            tunnel_accuracy = 0
            if np.any(fault_mask):
                tunnel_accuracy = accuracy_score(y_tunnel_test[fault_mask], tunnel_pred[fault_mask])
                print(f" å··é“å®šä½ä»»åŠ¡æ€§èƒ½ (ä»…æ•…éšœæ ·æœ¬):")
                print(f"   å‡†ç¡®ç‡: {tunnel_accuracy:.4f}")

            # è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
            if self.label_encoder:
                status_names = self.label_encoder.classes_
            else:
                status_names = ['æ­£å¸¸', 'æ•…éšœ']

            print("\n æ•…éšœè¯Šæ–­è¯¦ç»†æŠ¥å‘Š:")
            print(classification_report(y_status_test, status_pred, target_names=status_names, digits=4))

            # ç»˜åˆ¶å¢å¼ºçš„ç»“æœå¯è§†åŒ–
            self._plot_enhanced_results(y_status_test, status_pred, y_tunnel_test, tunnel_pred)

            return (status_pred, tunnel_pred), {
                'status_accuracy': status_accuracy,
                'status_precision': status_precision,
                'status_recall': status_recall,
                'status_f1': status_f1,
                'tunnel_accuracy': tunnel_accuracy if np.any(fault_mask) else 0
            }

        except Exception as e:
            print(f"âŒ æ¨¡å‹è¯„ä¼°å¤±è´¥: {e}")
            raise

    def _plot_enhanced_results(self, y_status_true, y_status_pred, y_tunnel_true, y_tunnel_pred):
        """ç»˜åˆ¶å¢å¼ºçš„å¤šä»»åŠ¡å­¦ä¹ ç»“æœ"""
        fig, axes = plt.subplots(2, 3, figsize=(24, 16))
        axes = axes.flatten()

        # 1. æ•…éšœè¯Šæ–­æ··æ·†çŸ©é˜µ
        cm_status = confusion_matrix(y_status_true, y_status_pred)
        sns.heatmap(cm_status, annot=True, fmt='d', cmap='Blues', ax=axes[0],
                    xticklabels=['æ­£å¸¸', 'æ•…éšœ'], yticklabels=['æ­£å¸¸', 'æ•…éšœ'])
        axes[0].set_title('æ•…éšœè¯Šæ–­æ··æ·†çŸ©é˜µ', fontsize=14, fontweight='bold')
        axes[0].set_xlabel('é¢„æµ‹æ ‡ç­¾')
        axes[0].set_ylabel('çœŸå®æ ‡ç­¾')

        # 2. å··é“å®šä½æ··æ·†çŸ©é˜µï¼ˆä»…æ•…éšœæ ·æœ¬ï¼‰
        fault_mask = y_status_true == 1
        if np.any(fault_mask) and self.tunnel_encoder:
            cm_tunnel = confusion_matrix(y_tunnel_true[fault_mask], y_tunnel_pred[fault_mask])
            tunnel_names = self.tunnel_encoder.classes_
            sns.heatmap(cm_tunnel, annot=True, fmt='d', cmap='Greens', ax=axes[1],
                        xticklabels=tunnel_names, yticklabels=tunnel_names)
            axes[1].set_title('å··é“å®šä½æ··æ·†çŸ©é˜µ (ä»…æ•…éšœæ ·æœ¬)', fontsize=14, fontweight='bold')
            axes[1].set_xlabel('é¢„æµ‹å··é“')
            axes[1].set_ylabel('çœŸå®å··é“')
            plt.setp(axes[1].get_xticklabels(), rotation=45, ha='right')
            plt.setp(axes[1].get_yticklabels(), rotation=0)
        else:
            axes[1].text(0.5, 0.5, 'æ— æ•…éšœæ ·æœ¬æ•°æ®', ha='center', va='center', transform=axes[1].transAxes)
            axes[1].set_title('å··é“å®šä½æ··æ·†çŸ©é˜µ', fontsize=14, fontweight='bold')

        # 3. è®­ç»ƒå†å² - å‡†ç¡®ç‡
        if self.history is not None:
            epochs = range(1, len(self.history.history['status_output_accuracy']) + 1)
            axes[2].plot(epochs, self.history.history['status_output_accuracy'],
                         label='æ•…éšœè¯Šæ–­è®­ç»ƒå‡†ç¡®ç‡', linewidth=2)
            axes[2].plot(epochs, self.history.history['val_status_output_accuracy'],
                         label='æ•…éšœè¯Šæ–­éªŒè¯å‡†ç¡®ç‡', linewidth=2)
            if 'tunnel_output_accuracy' in self.history.history:
                axes[2].plot(epochs, self.history.history['tunnel_output_accuracy'],
                             label='å··é“å®šä½è®­ç»ƒå‡†ç¡®ç‡', linewidth=2, linestyle='--')
                axes[2].plot(epochs, self.history.history['val_tunnel_output_accuracy'],
                             label='å··é“å®šä½éªŒè¯å‡†ç¡®ç‡', linewidth=2, linestyle='--')
            axes[2].set_title('è®­ç»ƒå†å² - å‡†ç¡®ç‡', fontsize=14, fontweight='bold')
            axes[2].set_xlabel('è®­ç»ƒè½®æ•°')
            axes[2].set_ylabel('å‡†ç¡®ç‡')
            axes[2].legend()
            axes[2].grid(True, alpha=0.3)

        # 4. è®­ç»ƒå†å² - æŸå¤±
        if self.history is not None:
            axes[3].plot(epochs, self.history.history['loss'],
                         label='æ€»è®­ç»ƒæŸå¤±', linewidth=2)
            axes[3].plot(epochs, self.history.history['val_loss'],
                         label='æ€»éªŒè¯æŸå¤±', linewidth=2)
            axes[3].set_title('è®­ç»ƒå†å² - æŸå¤±', fontsize=14, fontweight='bold')
            axes[3].set_xlabel('è®­ç»ƒè½®æ•°')
            axes[3].set_ylabel('æŸå¤±å€¼')
            axes[3].legend()
            axes[3].grid(True, alpha=0.3)

        # 5. æ•…éšœæ£€æµ‹æ¦‚ç‡åˆ†å¸ƒ
        fault_indices = np.where(y_status_true == 1)[0]
        normal_indices = np.where(y_status_true == 0)[0]

        if len(fault_indices) > 0 and len(normal_indices) > 0:
            predictions = self.model.predict(X_test, verbose=0)
            status_pred_proba, _ = predictions
            fault_probs = status_pred_proba[:, 1]

            axes[4].hist(fault_probs[normal_indices], bins=30, alpha=0.7, label='æ­£å¸¸æ ·æœ¬', color='green')
            axes[4].hist(fault_probs[fault_indices], bins=30, alpha=0.7, label='æ•…éšœæ ·æœ¬', color='red')
            axes[4].set_title('æ•…éšœæ£€æµ‹æ¦‚ç‡åˆ†å¸ƒ', fontsize=14, fontweight='bold')
            axes[4].set_xlabel('æ•…éšœæ¦‚ç‡')
            axes[4].set_ylabel('æ ·æœ¬æ•°é‡')
            axes[4].legend()
            axes[4].grid(True, alpha=0.3)

        # 6. ç‰¹å¾é‡è¦æ€§åˆ†æï¼ˆç®€åŒ–ç‰ˆï¼‰
        if hasattr(self, 'feature_importance'):
            feature_names = getattr(self, 'feature_names', [f'Feature_{i}' for i in range(10)])
            top_features = min(10, len(feature_names))
            indices = np.argsort(self.feature_importance)[-top_features:]

            axes[5].barh(range(top_features), self.feature_importance[indices])
            axes[5].set_yticks(range(top_features))
            axes[5].set_yticklabels([feature_names[i] for i in indices])
            axes[5].set_title('Top 10 é‡è¦ç‰¹å¾', fontsize=14, fontweight='bold')
            axes[5].set_xlabel('ç‰¹å¾é‡è¦æ€§')

        plt.tight_layout()
        plt.show()

        # ä¿å­˜ç»“æœå›¾
        save_path = "D:/Project_python/enhanced_multi_task_results.png"
        plt.savefig(save_path, dpi=300, bbox_inches='tight',
                    facecolor='white', edgecolor='none')
        print(f" å¢å¼ºçš„å¤šä»»åŠ¡ç»“æœå›¾å·²ä¿å­˜ä¸º '{save_path}'")

    def save_model(self, file_path=None):
        """ä¿å­˜æ¨¡å‹"""
        if file_path is None:
            model_dir = "D:/Project_python"
            os.makedirs(model_dir, exist_ok=True)
            file_path = os.path.join(model_dir, "enhanced_ventilation_multi_task_model.keras")

        self.model.save(file_path)
        print(f" å¢å¼ºçš„å¤šä»»åŠ¡æ¨¡å‹å·²ä¿å­˜åˆ°: {file_path}")

    def load_model(self, file_path=None):
        """åŠ è½½æ¨¡å‹"""
        if file_path is None:
            file_path = "D:/Project_python/enhanced_ventilation_multi_task_model.keras"

        self.model = load_model(file_path)
        print(f" æ¨¡å‹å·²ä» {file_path} åŠ è½½")


# ==================== å¢å¼ºçš„å®æ—¶è¯Šæ–­ç±» ====================
class EnhancedRealTimeDiagnosis:
    """å¢å¼ºçš„å®æ—¶æ•…éšœè¯Šæ–­ç±» - ä¸“é—¨å¤„ç†é£é˜»ä¼ æ’­æ•ˆåº”"""

    def __init__(self, model, data_processor):
        self.model = model
        self.data_processor = data_processor
        self.data_buffer = []
        self.confidence_history = []
        self.fault_probability_history = []
        self.recent_tunnel_predictions = []
        self.wind_speed_trends = {}
        self.resistance_anomalies = {}

    def add_data(self, new_data):
        """æ·»åŠ æ–°æ•°æ®åˆ°ç¼“å†²åŒº"""
        self.data_buffer.append(new_data)

        if len(self.data_buffer) > self.data_processor.sequence_length * 2:
            self.data_buffer = self.data_buffer[-self.data_processor.sequence_length * 2:]

    def analyze_wind_speed_trends(self, sequence_data):
        """åˆ†æé£é€Ÿè¶‹åŠ¿ç‰¹å¾"""
        trends = {}

        # è®¡ç®—å„å··é“çš„é£é€Ÿå˜åŒ–è¶‹åŠ¿
        for i in range(sequence_data.shape[1]):
            # å‡è®¾é£é€Ÿæ•°æ®åœ¨ç‰¹å®šä½ç½®
            wind_speed_data = sequence_data[:, i]
            if len(wind_speed_data) > 1:
                # è®¡ç®—è¶‹åŠ¿æ–œç‡
                x = np.arange(len(wind_speed_data))
                slope, _ = np.polyfit(x, wind_speed_data, 1)
                trends[f'feature_{i}_trend'] = slope

        return trends

    def detect_resistance_anomalies(self, sequence_data):
        """æ£€æµ‹é£é˜»å¼‚å¸¸æ¨¡å¼"""
        anomalies = {}

        # è®¡ç®—é£é˜»ç‰¹å¾çš„ç»Ÿè®¡å¼‚å¸¸
        resistance_features = [i for i, name in enumerate(self.data_processor.feature_names)
                               if 'é£é˜»' in name]

        for feature_idx in resistance_features:
            feature_data = sequence_data[:, feature_idx]
            mean_val = np.mean(feature_data)
            std_val = np.std(feature_data)

            # æ£€æµ‹å¼‚å¸¸ç‚¹
            z_scores = np.abs((feature_data - mean_val) / (std_val + 1e-8))
            anomaly_count = np.sum(z_scores > 2.0)
            anomalies[f'resistance_{feature_idx}_anomalies'] = anomaly_count / len(feature_data)

        return anomalies

    def diagnose_with_enhanced_location(self, confidence_threshold=0.7):
        """åŒ…å«å¢å¼ºå··é“å®šä½çš„å®æ—¶è¯Šæ–­"""
        if len(self.data_buffer) < self.data_processor.sequence_length:
            return "æ•°æ®ä¸è¶³", 0.0, "æœªçŸ¥", 0.0, {"error": "æ•°æ®ä¸è¶³"}

        try:
            sequence_data = np.array(self.data_buffer[-self.data_processor.sequence_length:])
            sequence_scaled = self.data_processor.scaler.transform(sequence_data)
            sequence_reshaped = sequence_scaled.reshape(1, self.data_processor.sequence_length, -1)

            # åˆ†æè¾…åŠ©ç‰¹å¾
            wind_trends = self.analyze_wind_speed_trends(sequence_data)
            resistance_anomalies = self.detect_resistance_anomalies(sequence_data)

            # å¤šä»»åŠ¡é¢„æµ‹
            predictions = self.model.model.predict(sequence_reshaped, verbose=0)
            status_pred_proba, tunnel_pred_proba = predictions

            fault_probability = status_pred_proba[0][1]
            confidence = max(fault_probability, 1 - fault_probability)

            # æ•…éšœè¯Šæ–­ç»“æœ
            if fault_probability > 0.5:
                diagnosis_result = "æ•…éšœ"
                # å··é“å®šä½
                tunnel_pred = np.argmax(tunnel_pred_proba[0])
                tunnel_confidence = tunnel_pred_proba[0][tunnel_pred]

                if self.data_processor.tunnel_encoder:
                    predicted_tunnel = self.data_processor.tunnel_encoder.inverse_transform([tunnel_pred])[0]
                    # å¦‚æœæ˜¯"æ— æ•…éšœ"ï¼Œé‡æ–°é€‰æ‹©ç¬¬äºŒå¯èƒ½çš„å··é“
                    if predicted_tunnel == 'æ— æ•…éšœ':
                        sorted_indices = np.argsort(tunnel_pred_proba[0])[::-1]
                        for idx in sorted_indices[1:]:
                            alternative_tunnel = self.data_processor.tunnel_encoder.inverse_transform([idx])[0]
                            if alternative_tunnel != 'æ— æ•…éšœ':
                                predicted_tunnel = alternative_tunnel
                                tunnel_confidence = tunnel_pred_proba[0][idx]
                                break
                else:
                    predicted_tunnel = f"å··é“_{tunnel_pred}"
            else:
                diagnosis_result = "æ­£å¸¸"
                predicted_tunnel = "æ— æ•…éšœ"
                tunnel_confidence = 1 - fault_probability

            # æ›´æ–°å†å²è®°å½•
            self.confidence_history.append(confidence)
            self.fault_probability_history.append(fault_probability)
            if diagnosis_result == "æ•…éšœ":
                self.recent_tunnel_predictions.append(predicted_tunnel)

            if len(self.confidence_history) > 10:
                self.confidence_history.pop(0)
                self.fault_probability_history.pop(0)
            if len(self.recent_tunnel_predictions) > 5:
                self.recent_tunnel_predictions.pop(0)

            # åˆ†æç»“æœç¨³å®šæ€§
            confidence_stability = np.std(self.confidence_history) if len(self.confidence_history) > 1 else 0

            # å··é“å®šä½ä¸€è‡´æ€§æ£€æŸ¥
            tunnel_consistency = 0
            if len(self.recent_tunnel_predictions) >= 3:
                tunnel_counts = Counter(self.recent_tunnel_predictions)
                most_common_tunnel, count = tunnel_counts.most_common(1)[0]
                tunnel_consistency = count / len(self.recent_tunnel_predictions)

            # ç”Ÿæˆå¢å¼ºçš„è¯Šæ–­æŠ¥å‘Š
            warnings = []
            recommendations = []

            if confidence < confidence_threshold:
                warnings.append(f"âš ï¸ è¯Šæ–­ç½®ä¿¡åº¦è¾ƒä½ ({confidence:.3f})")

            if confidence_stability > 0.1:
                warnings.append(f" ç½®ä¿¡åº¦æ³¢åŠ¨è¾ƒå¤§ ({confidence_stability:.3f})")

            if diagnosis_result == "æ•…éšœ" and tunnel_consistency < 0.6:
                warnings.append(f" å··é“å®šä½ä¸ä¸€è‡´ ({tunnel_consistency:.2f})")
                recommendations.append("å»ºè®®æ£€æŸ¥ç›¸é‚»å··é“çš„é£é˜»æƒ…å†µ")

            # åŸºäºé£é˜»å¼‚å¸¸çš„åˆ†æ
            high_anomaly_features = [k for k, v in resistance_anomalies.items() if v > 0.3]
            if high_anomaly_features and diagnosis_result == "æ•…éšœ":
                warnings.append(f" æ£€æµ‹åˆ°é£é˜»å¼‚å¸¸ç‰¹å¾: {len(high_anomaly_features)}ä¸ª")
                recommendations.append("é£é˜»å¼‚å¸¸å¯èƒ½è¡¨æ˜å±€éƒ¨é˜»å¡æˆ–å˜å½¢")

            # é£é€Ÿè¶‹åŠ¿åˆ†æ
            negative_trends = [k for k, v in wind_trends.items() if v < -0.1]
            if negative_trends and diagnosis_result == "æ•…éšœ":
                warnings.append(f" æ£€æµ‹åˆ°é£é€Ÿä¸‹é™è¶‹åŠ¿: {len(negative_trends)}ä¸ªç‰¹å¾")
                recommendations.append("é£é€Ÿä¸‹é™å¯èƒ½è¡¨æ˜é£é˜»å¢åŠ ")

            print(f" å®æ—¶è¯Šæ–­ç»“æœ: {diagnosis_result}")
            print(f" æ•…éšœæ¦‚ç‡: {fault_probability:.4f}")
            print(f" è¯Šæ–­ç½®ä¿¡åº¦: {confidence:.4f}")
            if diagnosis_result == "æ•…éšœ":
                print(f" é¢„æµ‹æ•…éšœå··é“: {predicted_tunnel}")
                print(f" å··é“å®šä½ç½®ä¿¡åº¦: {tunnel_confidence:.4f}")
                print(f" å··é“å®šä½ä¸€è‡´æ€§: {tunnel_consistency:.4f}")

            if warnings:
                print(" è­¦å‘Šä¿¡æ¯:")
                for warning in warnings:
                    print(f"  {warning}")

            if recommendations:
                print(" å¤„ç†å»ºè®®:")
                for recommendation in recommendations:
                    print(f"  {recommendation}")

            details = {
                'fault_probability': fault_probability,
                'confidence_stability': confidence_stability,
                'tunnel_consistency': tunnel_consistency,
                'wind_trends': wind_trends,
                'resistance_anomalies': resistance_anomalies,
                'warnings': warnings,
                'recommendations': recommendations,
                'recent_tunnel_predictions': self.recent_tunnel_predictions.copy(),
                'buffer_size': len(self.data_buffer)
            }

            return diagnosis_result, confidence, predicted_tunnel, tunnel_confidence, details

        except Exception as e:
            print(f"âŒ å®æ—¶è¯Šæ–­å¤±è´¥: {e}")
            return "è¯Šæ–­å¤±è´¥", 0.0, "æœªçŸ¥", 0.0, {"error": str(e)}


# ==================== ä¸»ç¨‹åº ====================
def main():
    """ä¸»å‡½æ•° - æ‰§è¡ŒåŸºäºé£é€Ÿè®¡ç®—é£é˜»çš„çŸ¿äº•é€šé£æ•…éšœè¯Šæ–­"""
    print("\n" + "=" * 70)
    print(" åŸºäºé£é€Ÿè®¡ç®—é£é˜»çš„çŸ¿äº•é€šé£æ•…éšœè¯Šæ–­ç³»ç»Ÿ")
    print("=" * 70)
    print(f" å·¥ä½œç›®å½•: D:/Project_python/")
    print(" ç³»ç»Ÿç‰¹æ€§:")
    print("  - åŸºäºé£é€Ÿã€é£å‹ã€æ–­é¢é¢ç§¯è®¡ç®—é£é˜»")
    print("  - è€ƒè™‘é£é˜»å˜åŒ–çš„ä¼ æ’­æ•ˆåº”")
    print("  - å¢å¼ºçš„CNN+LSTMå¤šä»»åŠ¡å­¦ä¹ æ¨¡å‹")
    print("  - å®æ—¶é£é˜»å¼‚å¸¸æ£€æµ‹å’Œè¶‹åŠ¿åˆ†æ")
    print("  - å¯è§†åŒ–æ‹“æ‰‘ç»“æ„å’Œè¯Šæ–­ç»“æœ")

    try:
        # 1. åˆå§‹åŒ–æ‹“æ‰‘ç»“æ„
        print("\n  æ­¥éª¤1: åˆå§‹åŒ–çŸ¿äº•æ‹“æ‰‘ç»“æ„")
        topology = MineTopology()
        tunnels, graph = topology.initialize_standard_topology()
        topology.visualize_topology()

        # 2. æ•°æ®å‡†å¤‡
        print("\n æ­¥éª¤2: å‡†å¤‡åŸºäºé£é€Ÿè®¡ç®—é£é˜»çš„æ•°æ®")
        processor = VentilationDataProcessor(sequence_length=60)
        data = processor.load_data()

        # æ•°æ®é¢„å¤„ç†
        X, y_status, y_tunnel = processor.preprocess_data(data)

        # åˆ›å»ºåºåˆ—æ•°æ®
        X_seq, y_status_seq, y_tunnel_seq = processor.create_sequences(X, y_status, y_tunnel, step_size=10)

        # æ•°æ®åˆ†å‰²
        X_train, X_test, y_status_train, y_status_test, y_tunnel_train, y_tunnel_test = train_test_split(
            X_seq, y_status_seq, y_tunnel_seq, test_size=0.15, random_state=42, stratify=y_status_seq
        )
        X_train, X_val, y_status_train, y_status_val, y_tunnel_train, y_tunnel_val = train_test_split(
            X_train, y_status_train, y_tunnel_train, test_size=0.15, random_state=42, stratify=y_status_train
        )

        print(f" æ•°æ®åˆ†å‰²å®Œæˆ:")
        print(f"  è®­ç»ƒé›†: {X_train.shape}")
        print(f"  éªŒè¯é›†: {X_val.shape}")
        print(f"  æµ‹è¯•é›†: {X_test.shape}")

        # 3. å¢å¼ºçš„å¤šä»»åŠ¡æ¨¡å‹æ„å»º
        print("\n  æ­¥éª¤3: æ„å»ºå¢å¼ºçš„å¤šä»»åŠ¡å­¦ä¹ æ¨¡å‹")
        input_shape = (X_train.shape[1], X_train.shape[2])
        num_status_classes = 2
        num_tunnel_classes = len(processor.tunnel_encoder.classes_)

        model_builder = EnhancedMultiTaskCNNLSTMModel(input_shape, num_status_classes, num_tunnel_classes)
        model_builder.label_encoder = processor.label_encoder
        model_builder.tunnel_encoder = processor.tunnel_encoder
        model = model_builder.build_enhanced_model(learning_rate=0.001)

        # 4. æ¨¡å‹è®­ç»ƒ
        print("\n æ­¥éª¤4: å¢å¼ºçš„å¤šä»»åŠ¡æ¨¡å‹è®­ç»ƒ")
        history = model_builder.train_with_enhanced_validation(
            X_train, y_status_train, y_tunnel_train,
            X_val, y_status_val, y_tunnel_val,
            epochs=150,
            batch_size=32
        )

        # 5. æ¨¡å‹è¯„ä¼°
        print("\n æ­¥éª¤5: å¢å¼ºçš„å¤šä»»åŠ¡æ¨¡å‹è¯„ä¼°")
        predictions, metrics = model_builder.comprehensive_evaluate(X_test, y_status_test, y_tunnel_test)

        # 6. ä¿å­˜æ¨¡å‹
        print("\n æ­¥éª¤6: ä¿å­˜æ¨¡å‹å’Œé¢„å¤„ç†å™¨")
        model_builder.save_model()
        processor.save_preprocessor()

        # 7. å®æ—¶è¯Šæ–­æ¼”ç¤º
        print("\n æ­¥éª¤7: å¢å¼ºçš„å®æ—¶è¯Šæ–­æ¼”ç¤º")
        real_time_diagnoser = EnhancedRealTimeDiagnosis(model_builder, processor)

        # ä½¿ç”¨æµ‹è¯•æ•°æ®è¿›è¡Œæ¼”ç¤º
        demo_samples = X_test[:30]
        for i, sample in enumerate(demo_samples):
            for time_point in sample[-5:]:
                real_time_diagnoser.add_data(time_point)

            # æ¯5ä¸ªæ ·æœ¬è¿›è¡Œä¸€æ¬¡è¯Šæ–­
            if i % 5 == 4:
                print(f"\n--- å¢å¼ºè¯Šæ–­æµ‹è¯• {i // 5 + 1} ---")
                diagnosis, confidence, tunnel, tunnel_confidence, details = real_time_diagnoser.diagnose_with_enhanced_location()

                if diagnosis == "æ•…éšœ":
                    print(f" æ£€æµ‹åˆ°æ•…éšœï¼ä½ç½®: {tunnel} (ç½®ä¿¡åº¦: {tunnel_confidence:.3f})")
                    if details.get('resistance_anomalies'):
                        print(
                            f" é£é˜»å¼‚å¸¸æ£€æµ‹: {sum(1 for v in details['resistance_anomalies'].values() if v > 0.3)}ä¸ªç‰¹å¾å¼‚å¸¸")
                else:
                    print(f" ç³»ç»Ÿæ­£å¸¸")

        print(f"\n åŸºäºé£é€Ÿè®¡ç®—é£é˜»çš„çŸ¿äº•é€šé£æ•…éšœè¯Šæ–­ç³»ç»Ÿå®Œæˆ!")
        print(f" æ•…éšœè¯Šæ–­å‡†ç¡®ç‡: {metrics['status_accuracy']:.4f}")
        print(f" å··é“å®šä½å‡†ç¡®ç‡: {metrics.get('tunnel_accuracy', 0):.4f}")

        # æ˜¾ç¤ºä¿å­˜çš„æ–‡ä»¶
        print(f"\n ç”Ÿæˆçš„æ–‡ä»¶:")
        project_dir = "D:/Project_python"
        if os.path.exists(project_dir):
            files = os.listdir(project_dir)
            for file in files:
                if file.endswith(('.keras', '.pkl', '.png', '.xls')):
                    print(f"  - {file}")

    except Exception as e:
        print(f" ç³»ç»Ÿè¿è¡Œå¤±è´¥: {e}")
        traceback.print_exc()


if __name__ == "__main__":
    main()